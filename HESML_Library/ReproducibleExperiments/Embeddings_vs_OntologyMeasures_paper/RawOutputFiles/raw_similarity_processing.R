# Description:
#
# This script loads a collection of word similarity benchmarks generated
# by HESML, whcih contain the raw similarity values for each word pair.
# Then, the script computes a consolidated table including the Pearson and
# Spearman correlation metrics together with the harmonic score of the
# former ones.
#
# References:
# ----------

# We clear all session variables

rm(list = ls())

# IMPORTANT: configuration of the input/output directories
# We define below the input directory for the input raw results
# in CSV file format, and the output directory for the
# final assembled tables in CSV file format.
# You must change these values in order to
# point to the proper directories in your hard drive.
# We also define below the name of the input raw CSV files
# containing the experimental results.

# The input and output directories below must end with '/' in
# Unix-like format to be compatible with Windows
# or Linux-based R distributions.

inputDir = "C:/Versiones_Git_Oficiales_HESML/HESML/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper/RawOutputFiles/"
outputDir = inputDir

# Input raw CSV files generated by the reproducible experiments detailed
# in the paper.

raw_MC28_file = "raw_similarity_values_MC28_dataset.csv"
raw_RG65_file = "raw_similarity_values_RG65_dataset.csv"
raw_PSfull_file = "raw_similarity_values_PSfull_dataset.csv"
raw_Agirre201_file = "raw_similarity_values_Agirre201_lowercase_dataset.csv"
raw_SimLex665_file = "raw_similarity_values_SimLex665_dataset.csv"
raw_MTurk771_file = "raw_similarity_values_MTurk771_dataset.csv"
raw_MTurk287_235_file = "raw_similarity_values_MTurk287-235_dataset.csv"
raw_WS353Rel_file = "raw_similarity_values_WS353Rel_dataset.csv"
raw_Rel122_file = "raw_similarity_values_Rel122_dataset.csv"
raw_WS353Full_file = "raw_similarity_values_WS353Full_dataset.csv"
raw_SimLex111_file = "raw_similarity_values_SimLex111_dataset.csv"
raw_SimLex222_file = "raw_similarity_values_SimLex222_dataset.csv"
raw_SimLex999_file = "raw_similarity_values_SimLex999_dataset.csv"
raw_SimVerb3500_file = "raw_similarity_values_SimVerb3500_dataset.csv"
raw_MEN_file = "raw_similarity_values_MEN_dataset.csv"
raw_YP130_file = "raw_similarity_values_YP130_dataset.csv"
raw_RareWords2034_file = "raw_similarity_values_RareWords2034_dataset.csv"
raw_RareWords1401_file = "raw_similarity_values_RareWords1401_dataset.csv"
raw_SCWS1994_file = "raw_similarity_values_SCWS1994_dataset.csv"

# We load the input raw results file

rawdata_MC28<-read.csv(paste(inputDir, sep = "", raw_MC28_file),dec = ".", sep = ';')
rawdata_RG65<-read.csv(paste(inputDir, sep = "", raw_RG65_file),dec = ".", sep = ';')
rawdata_PSfull<-read.csv(paste(inputDir, sep = "", raw_PSfull_file),dec = ".", sep = ';')
rawdata_Agirre201<-read.csv(paste(inputDir, sep = "", raw_Agirre201_file),dec = ".", sep = ';')
rawdata_SimLex665<-read.csv(paste(inputDir, sep = "", raw_SimLex665_file),dec = ".", sep = ';')
rawdata_MTurk771<-read.csv(paste(inputDir, sep = "", raw_MTurk771_file),dec = ".", sep = ';')
rawdata_MTurk287_235<-read.csv(paste(inputDir, sep = "", raw_MTurk287_235_file),dec = ".", sep = ';')
rawdata_WS353Rel<-read.csv(paste(inputDir, sep = "", raw_WS353Rel_file),dec = ".", sep = ';')
rawdata_Rel122<-read.csv(paste(inputDir, sep = "", raw_Rel122_file),dec = ".", sep = ';')
rawdata_WS353Full<-read.csv(paste(inputDir, sep = "", raw_WS353Full_file),dec = ".", sep = ';')
rawdata_SimLex111<-read.csv(paste(inputDir, sep = "", raw_SimLex111_file),dec = ".", sep = ';')
rawdata_SimLex222<-read.csv(paste(inputDir, sep = "", raw_SimLex222_file),dec = ".", sep = ';')
rawdata_SimLex999<-read.csv(paste(inputDir, sep = "", raw_SimLex999_file),dec = ".", sep = ';')
rawdata_SimVerb3500<-read.csv(paste(inputDir, sep = "", raw_SimVerb3500_file),dec = ".", sep = ';')
rawdata_MEN<-read.csv(paste(inputDir, sep = "", raw_MEN_file),dec = ".", sep = ';')
rawdata_YP130<-read.csv(paste(inputDir, sep = "", raw_YP130_file),dec = ".", sep = ';')
rawdata_RareWords2034<-read.csv(paste(inputDir, sep = "", raw_RareWords2034_file),dec = ".", sep = ';')
rawdata_RareWords1401<-read.csv(paste(inputDir, sep = "", raw_RareWords1401_file),dec = ".", sep = ';')
rawdata_SCWS1994<-read.csv(paste(inputDir, sep = "", raw_SCWS1994_file),dec = ".", sep = ';')

# ---------------------------------------------------------------------
# Table 1: Pearson, Spearman and Harmonic mean metrics of all measures
# and embeddings in the 5 similarity datasets evaluated both by the
# ontoloy-based measures basedon Wordnet as the pre-trained word
# embedding models.
# ---------------------------------------------------------------------

# We define all datasets represented in table 1

rawdataSimNounDatasets = list(rawdata_MC28, rawdata_RG65, rawdata_PSfull, rawdata_Agirre201, rawdata_SimLex665)

# We create the table 1

table1<-matrix(nrow = 21, ncol = 3 * length(rawdataSimNounDatasets))

colnames(table1)<-c("MC28-Pearson", "MC28-Spearman", "MC28-Harmonic", "RG65-Pearson", "RG65-Spearman", "RG65-Harmonic", "PSfull-Pearson", "PSfull-Spearman", "PSfull-Harmonic", "Agirre201-Pearson", "Agirre201-Spearman", "Agirre201-Harmonic", "SimLex665-Pearson", "SimLex665-Spearman", "SimLex665-Harmonic");
rownames(table1)<-colnames(rawdata_MC28)[3:23]

nMeasures = nrow(table1)
nDatasets = length(rawdataSimNounDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata = rawdataSimNounDatasets[[iDataset]]

	# We evaluate the Pearson, Spearman and Harmonic metrics for each measure in the current dataset

	for (iMeasure in 1:nMeasures)
	{
		iColumn = 3 * (iDataset - 1) + 1
		table1[iMeasure, iColumn] = cor(rawdata[,2], rawdata[, iMeasure + 2], method = "pearson")
		table1[iMeasure, iColumn + 1] = cor(rawdata[,2], rawdata[, iMeasure + 2], method = "spearman")
		table1[iMeasure, iColumn + 2] = 2 * table1[iMeasure, iColumn] * table1[iMeasure, iColumn + 1] / (table1[iMeasure, iColumn] + table1[iMeasure, iColumn + 1])
	}
}

#We make a copy of the table in order to round its values to 3 decimal digits

table1_rounded = round(table1, 3);

# We save all the final assembled data tables 

write.csv(table1, file = paste(outputDir, sep="","table1_raw_values.csv"))
write.csv(table1_rounded, file = paste(outputDir, sep="","table1_rounded_values.csv"))

# ---------------------------------------------------------------------
# Table 2: Pearson, Spearman and Harmonic mean metrics of all measures
# and embeddings in the 4 relatedness datasets evaluated both by the
# ontoloy-based measures basedon Wordnet as the pre-trained word
# embedding models.
# ---------------------------------------------------------------------

# We define all datasets represented in table 2

rawdataRelNounDatasets = list(rawdata_MTurk771, rawdata_MTurk287_235, rawdata_WS353Rel, rawdata_Rel122)

# We create the table 2

table2<-matrix(nrow = 21, ncol = 3 * length(rawdataRelNounDatasets))

colnames(table2)<-c("MTurk771-Pearson", "MTurk771-Spearman", "MTurk771-Harmonic", "MTurk287_235-Pearson", "MTurk287_235-Spearman", "MTurk287_235-Harmonic", "WS353Rel-Pearson", "WS353Rel-Spearman", "WS353Rel-Harmonic", "Rel122-Pearson", "Rel122-Spearman", "Rel122-Harmonic");

rownames(table2)<-colnames(rawdata_MTurk771)[3:23]

nMeasures = nrow(table2)
nDatasets = length(rawdataRelNounDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata = rawdataRelNounDatasets[[iDataset]]

	# We evaluate the Pearson, Spearman and Harmonic metrics for each measure in the current dataset

	for (iMeasure in 1:nMeasures)
	{
		iColumn = 3 * (iDataset - 1) + 1
		table2[iMeasure, iColumn] = cor(rawdata[,2], rawdata[, iMeasure + 2], method = "pearson")
		table2[iMeasure, iColumn + 1] = cor(rawdata[,2], rawdata[, iMeasure + 2], method = "spearman")
		table2[iMeasure, iColumn + 2] = 2 * table2[iMeasure, iColumn] * table2[iMeasure, iColumn + 1] / (table2[iMeasure, iColumn] + table2[iMeasure, iColumn + 1])
	}
}

#We make a copy of the table in order to round its values to 3 decimal digits

table2_rounded = round(table2, 3);

# ---------------------------------------------------------------------
# Table 3, 4 and 5: Pearson, Spearman and Harmonic metrics of all
# pre-trained embeddings models in all datasets. Rows contain datasets
# whilst columns contain the perfromance of each word embedding model
# in each dataset. Columns are sorted in descending order from
# left to right. Leftmost columns show best performing embedding models.
# ---------------------------------------------------------------------

# We define all datasets represented in table 2

rawdataAllDatasets = list(rawdata_MC28, rawdata_RG65, rawdata_PSfull, rawdata_Agirre201, rawdata_SimLex665, rawdata_MTurk771, rawdata_MTurk287_235, rawdata_WS353Rel, rawdata_Rel122, rawdata_WS353Full, rawdata_SimLex111, rawdata_SimLex222, rawdata_SimLex999, rawdata_SimVerb3500, rawdata_MEN, rawdata_YP130, rawdata_RareWords2034, rawdata_RareWords1401, rawdata_SCWS1994);

# We create the tables 3,4 and 5

table3<-matrix(ncol = 11, nrow = length(rawdataAllDatasets))
table4<-matrix(ncol = 11, nrow = length(rawdataAllDatasets))
table5<-matrix(ncol = 11, nrow = length(rawdataAllDatasets))

rownames(table3)<-c("MC28", "RG65", "PSfull", "Agirre201", "SimLex665", "MTurk771", "MTurk287_235", "WS353Rel", "Rel122", "WS353Full", "SimLex111", "SimLex222", "SimLex999", "SimVerb3500", "MEN", "YP130", "RW2034", "RW1401", "SCWS1994");
colnames(table3)<-colnames(rawdata_MC28)[13:23]

rownames(table4)<-rownames(table3)
colnames(table4)<-colnames(table3)

rownames(table5)<-rownames(table3)
colnames(table5)<-colnames(table3)

nMeasures = ncol(table3)
nDatasets = length(rawdataAllDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata = rawdataAllDatasets[[iDataset]]

	# We evaluate the Pearson, Spearman and Harmonic metrics for each measure in the current dataset

	for (iMeasure in 1:nMeasures)
	{
		table3[iDataset, iMeasure] = cor(rawdata[,2], rawdata[, iMeasure + 2], method = "pearson")
		table4[iDataset, iMeasure] = cor(rawdata[,2], rawdata[, iMeasure + 2], method = "spearman")
		table5[iDataset, iMeasure] = 2 * table3[iDataset, iMeasure] * table4[iDataset, iMeasure] / (table3[iDataset, iMeasure] + table4[iDataset, iMeasure])
	}
}

# We transpose the matrix in order to compute the average of columns

table3 = rbind(table3, Avg = colMeans(table3[,1:ncol(table3)]))
table4 = rbind(table4, Avg = colMeans(table4[,1:ncol(table4)]))
table5 = rbind(table5, Avg = colMeans(table5[,1:ncol(table5)]))

# IMPORTANT: you must install the 'BioPhysConnectoR' package before to run the next three lines of code
# We sort the tables 3,4 and 5 in descending order by using the column-based Average values (last row)

library(BioPhysConnectoR)

# We transpose the matrices in order to sort them according to the
# average value obtained by each embedding model

table3 = t(table3)
table3 = mat.sort(table3, ncol(table3), decreasing = TRUE)
table3 = t(table3)

table4 = t(table4)
table4 = mat.sort(table4, ncol(table4), decreasing = TRUE)
table4 = t(table4)

table5 = t(table5)
table5 = mat.sort(table5, ncol(table5), decreasing = TRUE)
table5 = t(table5)

# We make a copy of the table in order to round its values to 3 decimal digits

table3_rounded = round(table3, 3);
table4_rounded = round(table4, 3);
table5_rounded = round(table5, 3);

# We save all the final assembled data tables 

write.csv(table3, file = paste(outputDir, sep="","table3_raw_values.csv"))
write.csv(table3_rounded, file = paste(outputDir, sep="","table3_rounded_values.csv"))

write.csv(table4, file = paste(outputDir, sep="","table4_raw_values.csv"))
write.csv(table4_rounded, file = paste(outputDir, sep="","table4_rounded_values.csv"))

write.csv(table5, file = paste(outputDir, sep="","table5_raw_values.csv"))
write.csv(table5_rounded, file = paste(outputDir, sep="","table5_rounded_values.csv"))


