# Description:
#
# This script loads a collection of word similarity benchmarks generated
# by HESML, ehich contain the raw similarity values for each word pair.
# Then, the script computes a collection of consolidated tables
# including the Pearson and Spearman correlation metrics together
# with the harmonic score of the # former ones.
#
# References:
# ----------

# We clear all session variables

rm(list = ls())

# IMPORTANT: configuration of the input/output directories
# We define below the input directory for the input raw results
# in CSV file format, and the output directory for the
# final assembled tables in CSV file format.
# You must change these values in order to
# point to the proper directories in your hard drive.
# We also define below the name of the input raw CSV files
# containing the experimental results.

# The input and output directories below must end with '/' in
# Unix-like format to be compatible with Windows
# or Linux-based R distributions.

#inputDir = "C:/Versiones_Git_Oficiales_HESML/HESML/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper/RawOutputFiles/"
#outputDir = "C:/Versiones_Git_Oficiales_HESML/HESML/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper/ProcessedOutputFiles/"

inputDir = "D:/Versiones_Git_Oficiales_HESML/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper/RawOutputFiles/"
outputDir = "D:/Versiones_Git_Oficiales_HESML/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper/ProcessedOutputFiles/"
dir.create(outputDir)

# Input raw CSV files generated by the reproducible experiments detailed
# in the companion paper.

raw_MC28_file <- "raw_similarity_values_MC28_dataset.csv"
raw_RG65_file <- "raw_similarity_values_RG65_dataset.csv"
raw_PSfull_file <- "raw_similarity_values_PSfull_dataset.csv"
raw_Agirre201_file <- "raw_similarity_values_Agirre201_lowercase_dataset.csv"
raw_SimLex665_file <- "raw_similarity_values_SimLex665_dataset.csv"
raw_MTurk771_file <- "raw_similarity_values_MTurk771_dataset.csv"
raw_MTurk287_235_file  <- "raw_similarity_values_MTurk287-235_dataset.csv"
raw_WS353Rel_file <- "raw_similarity_values_WS353Rel_dataset.csv"
raw_Rel122_file <- "raw_similarity_values_Rel122_dataset.csv"
raw_WS353Full_file <- "raw_similarity_values_WS353Full_dataset.csv"
raw_SimLex111_file <- "raw_similarity_values_SimLex111_dataset.csv"
raw_SimLex222_file <- "raw_similarity_values_SimLex222_dataset.csv"
raw_SimLex999_file <- "raw_similarity_values_SimLex999_dataset.csv"
raw_SimVerb3500_file <- "raw_similarity_values_SimVerb3500_dataset.csv"
raw_MEN_file <- "raw_similarity_values_MEN_dataset.csv"
raw_YP130_file <- "raw_similarity_values_YP130_dataset.csv"
raw_RareWords2034_file <- "raw_similarity_values_RareWords2034_dataset.csv"
raw_RareWords1401_file <- "raw_similarity_values_RareWords1401_dataset.csv"
raw_SCWS1994_file <- "raw_similarity_values_SCWS1994_dataset.csv"

# We load the input raw results file

rawdata_MC28 <- read.csv(paste(inputDir, sep = "", raw_MC28_file),dec = ".", sep = ';')
rawdata_RG65 <- read.csv(paste(inputDir, sep = "", raw_RG65_file),dec = ".", sep = ';')
rawdata_PSfull <- read.csv(paste(inputDir, sep = "", raw_PSfull_file),dec = ".", sep = ';')
rawdata_Agirre201 <- read.csv(paste(inputDir, sep = "", raw_Agirre201_file),dec = ".", sep = ';')
rawdata_SimLex665 <- read.csv(paste(inputDir, sep = "", raw_SimLex665_file),dec = ".", sep = ';')
rawdata_MTurk771 <- read.csv(paste(inputDir, sep = "", raw_MTurk771_file),dec = ".", sep = ';')
rawdata_MTurk287_235 <- read.csv(paste(inputDir, sep = "", raw_MTurk287_235_file),dec = ".", sep = ';')
rawdata_WS353Rel <- read.csv(paste(inputDir, sep = "", raw_WS353Rel_file),dec = ".", sep = ';')
rawdata_Rel122 <- read.csv(paste(inputDir, sep = "", raw_Rel122_file),dec = ".", sep = ';')
rawdata_WS353Full <- read.csv(paste(inputDir, sep = "", raw_WS353Full_file),dec = ".", sep = ';')
rawdata_SimLex111 <- read.csv(paste(inputDir, sep = "", raw_SimLex111_file),dec = ".", sep = ';')
rawdata_SimLex222 <- read.csv(paste(inputDir, sep = "", raw_SimLex222_file),dec = ".", sep = ';')
rawdata_SimLex999 <- read.csv(paste(inputDir, sep = "", raw_SimLex999_file),dec = ".", sep = ';')
rawdata_SimVerb3500 <- read.csv(paste(inputDir, sep = "", raw_SimVerb3500_file),dec = ".", sep = ';')
rawdata_MEN <- read.csv(paste(inputDir, sep = "", raw_MEN_file),dec = ".", sep = ';')
rawdata_YP130 <- read.csv(paste(inputDir, sep = "", raw_YP130_file),dec = ".", sep = ';')
rawdata_RareWords2034 <- read.csv(paste(inputDir, sep = "", raw_RareWords2034_file),dec = ".", sep = ';')
rawdata_RareWords1401 <- read.csv(paste(inputDir, sep = "", raw_RareWords1401_file),dec = ".", sep = ';')
rawdata_SCWS1994 <- read.csv(paste(inputDir, sep = "", raw_SCWS1994_file),dec = ".", sep = ';')

# IMPORTANT: you must install the 'BioPhysConnectoR' package before to run the next three lines of code
# We sort the tables 3,4 and 5 in descending order by using the column-based Average values (last row)

library(BioPhysConnectoR)

# ---------------------------------------------------------------------
# Raw output file format:
# Raw similarity files contain the similarity values returned by each
# semantic measure for each word pair. First nine dataset were evaluated
# with a set of 21 ontology-based measures and 11 word embedding models,
# whilst remaining datasets were only evaluated with the word embeddings.
# First two columns contain the word pairs and the human judgements,
# whilst subsequent columns contain the values returned by the
# twenty-one ontology-based measures and the eleven word embedding models.
# ---------------------------------------------------------------------

# ---------------------------------------------------------------------
# Tables 1,2 and 3: Pearson, Spearman and Harmonic mean metrics of all measures
# and embeddings in the 5 similarity datasets evaluated both by the
# ontology-based measures based on WordNet as the pre-trained word
# embedding models.
# ---------------------------------------------------------------------

# We define all datasets represented in table 1

rawdataSimNounDatasets <- list(rawdata_MC28, rawdata_RG65, rawdata_PSfull, rawdata_Agirre201, rawdata_SimLex665)

# We create a separated table for each metric by removing first two columns
# which contain the word pairs and human judgements. 
# Measures are aranged in rows whilst datasets are arranged in columns.

table_Pearson_SimDatasets <- matrix(nrow = ncol(rawdata_MC28) - 2,
                                  ncol = length(rawdataSimNounDatasets),
                                  dimnames = list(colnames(rawdata_MC28)[3:ncol(rawdata_MC28)],
                                                  c("MC28", "RG65", "PSfull", "Agirre201", "SimLex665")))

table_Spearman_SimDatasets <- table_Pearson_SimDatasets
table_Harmonic_SimDatasets <- table_Pearson_SimDatasets

# Loop for the computation of the metrics

nMeasures <- nrow(table_Pearson_SimDatasets)
nDatasets <- length(rawdataSimNounDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata <- rawdataSimNounDatasets[[iDataset]]

	# We evaluate the Pearson, Spearman and Harmonic metrics for each measure in the current dataset

	for (iMeasure in 1:nMeasures)
	{
		table_Pearson_SimDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "pearson")
		table_Spearman_SimDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "spearman")
		table_Harmonic_SimDatasets[iMeasure, iDataset] <- 2.0 * table_Pearson_SimDatasets[iMeasure, iDataset] * table_Spearman_SimDatasets[iMeasure, iDataset] / (table_Pearson_SimDatasets[iMeasure, iDataset] + table_Spearman_SimDatasets[iMeasure, iDataset])
	}
}

# We compute the average values per row and sort the rows

table_Pearson_SimDatasets <- cbind(table_Pearson_SimDatasets, Avg = rowMeans(table_Pearson_SimDatasets[1:nrow(table_Pearson_SimDatasets),]))
table_Pearson_SimDatasets <- mat.sort(table_Pearson_SimDatasets, ncol(table_Pearson_SimDatasets), decreasing = TRUE)

table_Spearman_SimDatasets <- cbind(table_Spearman_SimDatasets, Avg = rowMeans(table_Spearman_SimDatasets[1:nrow(table_Spearman_SimDatasets),]))
table_Spearman_SimDatasets <- mat.sort(table_Spearman_SimDatasets, ncol(table_Spearman_SimDatasets), decreasing = TRUE)

table_Harmonic_SimDatasets <- cbind(table_Harmonic_SimDatasets, Avg = rowMeans(table_Harmonic_SimDatasets[1:nrow(table_Harmonic_SimDatasets),]))
table_Harmonic_SimDatasets <- mat.sort(table_Harmonic_SimDatasets, ncol(table_Harmonic_SimDatasets), decreasing = TRUE)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_Pearson_SimDatasets_rounded <- round(table_Pearson_SimDatasets, 3);
table_Spearman_SimDatasets_rounded <- round(table_Spearman_SimDatasets, 3);
table_Harmonic_SimDatasets_rounded <- round(table_Harmonic_SimDatasets, 3);

# We save all final assembled data tables 

write.csv(table_Pearson_SimDatasets, file = paste(outputDir, sep="","table_Pearson_SimDatasets.csv"))
write.csv(table_Pearson_SimDatasets_rounded, file = paste(outputDir, sep="","table_Pearson_SimDatasets_rounded.csv"))

write.csv(table_Spearman_SimDatasets, file = paste(outputDir, sep="","table_Spearman_SimDatasets.csv"))
write.csv(table_Spearman_SimDatasets_rounded, file = paste(outputDir, sep="","table_Spearman_SimDatasets_rounded.csv"))

write.csv(table_Harmonic_SimDatasets, file = paste(outputDir, sep="","table_Harmonic_SimDatasets.csv"))
write.csv(table_Harmonic_SimDatasets_rounded, file = paste(outputDir, sep="","table_Harmonic_SimDatasets_rounded.csv"))

# ---------------------------------------------------------------------
# Table 4,5 and 6: Pearson, Spearman and Harmonic mean metrics of all measures
# and embeddings in the 4 relatedness datasets evaluated both by the
# ontoloy-based measures basedon Wordnet as the pre-trained word
# embedding models.
# ---------------------------------------------------------------------

# We define all relatedness datasets represented in tables

rawdataRelNounDatasets <- list(rawdata_MTurk771, rawdata_MTurk287_235, rawdata_WS353Rel, rawdata_Rel122)

# We create the table 2

# We create a separated table for each metric

table_Pearson_RelDatasets <- matrix(nrow = ncol(rawdata_MC28) - 2,
                                    ncol = length(rawdataRelNounDatasets),
                                    dimnames = list(colnames(rawdata_MC28)[3:ncol(rawdata_MC28)],
                                                    c("MTurk771", "MTurk287_235", "WS353Rel", "Rel122")))

table_Spearman_RelDatasets <- table_Pearson_RelDatasets
table_Harmonic_RelDatasets <- table_Pearson_RelDatasets

# Loop for the computation of all metrics

nMeasures <- nrow(table_Pearson_RelDatasets)
nDatasets <- length(rawdataRelNounDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata <- rawdataRelNounDatasets[[iDataset]]

	# We evaluate the Pearson, Spearman and Harmonic metrics for each measure in the current dataset

	for (iMeasure in 1:nMeasures)
	{
		table_Pearson_RelDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "pearson")
		table_Spearman_RelDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "spearman")
		table_Harmonic_RelDatasets[iMeasure, iDataset] <- 2.0 * table_Pearson_RelDatasets[iMeasure, iDataset] * table_Spearman_RelDatasets[iMeasure, iDataset] / (table_Pearson_RelDatasets[iMeasure, iDataset] + table_Spearman_RelDatasets[iMeasure, iDataset])
	}
}

# We compute the average values per row and sort the rows

table_Pearson_RelDatasets <- cbind(table_Pearson_RelDatasets, Avg = rowMeans(table_Pearson_RelDatasets[1:nrow(table_Pearson_RelDatasets),]))
table_Pearson_RelDatasets <- mat.sort(table_Pearson_RelDatasets, ncol(table_Pearson_RelDatasets), decreasing = TRUE)

table_Spearman_RelDatasets <- cbind(table_Spearman_RelDatasets, Avg = rowMeans(table_Spearman_RelDatasets[1:nrow(table_Spearman_RelDatasets),]))
table_Spearman_RelDatasets <- mat.sort(table_Spearman_RelDatasets, ncol(table_Spearman_RelDatasets), decreasing = TRUE)

table_Harmonic_RelDatasets <- cbind(table_Harmonic_RelDatasets, Avg = rowMeans(table_Harmonic_RelDatasets[1:nrow(table_Harmonic_RelDatasets),]))
table_Harmonic_RelDatasets <- mat.sort(table_Harmonic_RelDatasets, ncol(table_Harmonic_RelDatasets), decreasing = TRUE)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_Pearson_RelDatasets_rounded <- round(table_Pearson_RelDatasets, 3);
table_Spearman_RelDatasets_rounded <- round(table_Spearman_RelDatasets, 3);
table_Harmonic_RelDatasets_rounded <- round(table_Harmonic_RelDatasets, 3);

# We save all final assembled data tables 

write.csv(table_Pearson_RelDatasets, file = paste(outputDir, sep="","table_Pearson_RelDatasets.csv"))
write.csv(table_Pearson_RelDatasets_rounded, file = paste(outputDir, sep="","table_Pearson_RelDatasets_rounded.csv"))

write.csv(table_Spearman_RelDatasets, file = paste(outputDir, sep="","table_Spearman_RelDatasets.csv"))
write.csv(table_Spearman_RelDatasets_rounded, file = paste(outputDir, sep="","table_Spearman_RelDatasets_rounded.csv"))

write.csv(table_Harmonic_RelDatasets, file = paste(outputDir, sep="","table_Harmonic_RelDatasets.csv"))
write.csv(table_Harmonic_RelDatasets_rounded, file = paste(outputDir, sep="","table_Harmonic_RelDatasets_rounded.csv"))

# ---------------------------------------------------------------------
# Table 7, 8 and 9: Pearson, Spearman and Harmonic metrics of all
# pre-trained embeddings models in all datasets. Rows contain datasets
# whilst columns contain the perfromance of each word embedding model
# in each dataset. Columns are sorted in descending order from
# left to right. Leftmost columns show best performing embedding models.
# ---------------------------------------------------------------------

# We define all datasets

rawdataAllDatasets<-list(rawdata_MC28, rawdata_RG65, rawdata_PSfull, rawdata_Agirre201, rawdata_SimLex665, rawdata_SimLex111, rawdata_SimLex222, rawdata_SimLex999, rawdata_SimVerb3500, rawdata_MTurk771, rawdata_MTurk287_235, rawdata_WS353Rel, rawdata_Rel122, rawdata_WS353Full, rawdata_MEN, rawdata_YP130, rawdata_RareWords2034, rawdata_RareWords1401, rawdata_SCWS1994);
rawdataAllDatasetNames<-c("MC28", "RG65", "PSfull", "Agirre201", "SimLex665", "SimLex111", "SimLex222", "SimLex999", "SimVerb3500", "MTurk771", "MTurk287_235", "WS353Rel", "Rel122", "WS353Full", "MEN", "YP130", "RW2034", "RW1401", "SCWS1994")

# We create the tables 3,4 and 5

table_Pearson_allEmbeddings <- matrix(ncol = 11, nrow = length(rawdataAllDatasets),
                                      dimnames = list(rawdataAllDatasetNames,
                                                      colnames(rawdata_WS353Full)[3:ncol(rawdata_WS353Full)]))

table_Spearman_allEmbeddings <- table_Pearson_allEmbeddings
table_Harmonic_allEmbeddings <- table_Pearson_allEmbeddings

nOntologyBasedMeasures = 21;
nEmbeddings <- ncol(table_Pearson_allEmbeddings)
nDatasets <- length(rawdataAllDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata <- rawdataAllDatasets[[iDataset]]

	# We define the offset position to extract the column of each embedding model.
	# First nine raw input datasets include the evaluation of 21 ontology-based measures and
	# 11 embedding models. However, the remaining 10 datasets are only evaluated
	# on the embedding models.

	if ((iDataset <= 5) || ((iDataset >= 10) && (iDataset <= 13)))
	{
		iOffset <- nOntologyBasedMeasures + 2;
	}
	else
	{
		iOffset <- 2;
	}

	# We evaluate the Pearson, Spearman and Harmonic metrics for each
	# embeding model in the current dataset

	for (iEmbedding in 1:nEmbeddings)
	{
		table_Pearson_allEmbeddings[iDataset, iEmbedding] <- cor(rawdata[,2], rawdata[, iEmbedding + iOffset], method = "pearson")
		table_Spearman_allEmbeddings[iDataset, iEmbedding] <- cor(rawdata[,2], rawdata[, iEmbedding + iOffset], method = "spearman")
		table_Harmonic_allEmbeddings[iDataset, iEmbedding] <- 2 * table_Pearson_allEmbeddings[iDataset, iEmbedding] * table_Spearman_allEmbeddings[iDataset, iEmbedding] / (table_Pearson_allEmbeddings[iDataset, iEmbedding] + table_Spearman_allEmbeddings[iDataset, iEmbedding])
	}
}

#-----------------------------------------------------------------
# PERFORMANCE of word embeddings only in SIMILARITY DATASETS
# We extract the performance of all models in all similarity datasets
#-----------------------------------------------------------------

# We extract the similarity datasets

table_Pearson_allEmbeddings_similarity <- table_Pearson_allEmbeddings[1:9,]
table_Pearson_allEmbeddings_similarity <- rbind(table_Pearson_allEmbeddings_similarity, Avg_r = colMeans(table_Pearson_allEmbeddings_similarity[,1:ncol(table_Pearson_allEmbeddings_similarity)]))

table_Spearman_allEmbeddings_similarity <- table_Spearman_allEmbeddings[1:9,]
table_Spearman_allEmbeddings_similarity <- rbind(table_Spearman_allEmbeddings_similarity, Avg_rho = colMeans(table_Spearman_allEmbeddings_similarity[,1:ncol(table_Spearman_allEmbeddings_similarity)]))

table_Harmonic_allEmbeddings_similarity <- table_Harmonic_allEmbeddings[1:9,]
table_Harmonic_allEmbeddings_similarity <- rbind(table_Harmonic_allEmbeddings_similarity, Avg_h = colMeans(table_Harmonic_allEmbeddings_similarity[,1:ncol(table_Harmonic_allEmbeddings_similarity)]))

table_joined_allEmbeddings_similarity = rbind(table_Pearson_allEmbeddings_similarity, table_Spearman_allEmbeddings_similarity, table_Harmonic_allEmbeddings_similarity)

# We transpose the matrices in order to sort them according to the
# average value obtained by each embedding model

table_joined_allEmbeddings_similarity <- t(table_joined_allEmbeddings_similarity)
table_joined_allEmbeddings_similarity <- mat.sort(table_joined_allEmbeddings_similarity, ncol(table_joined_allEmbeddings_similarity), decreasing = TRUE)
table_joined_allEmbeddings_similarity <- t(table_joined_allEmbeddings_similarity)

#-----------------------------------------------------------------
# PERFORMANCE of word embeddings only in RELATEDNESS DATASETS
# We extract the performance of all models in all similarity datasets
#-----------------------------------------------------------------

table_Pearson_allEmbeddings_relatedness <- table_Pearson_allEmbeddings[10:nrow(table_Pearson_allEmbeddings),]
table_Pearson_allEmbeddings_relatedness <- rbind(table_Pearson_allEmbeddings_relatedness, Avg_r = colMeans(table_Pearson_allEmbeddings_relatedness[,1:ncol(table_Pearson_allEmbeddings_relatedness)]))

table_Spearman_allEmbeddings_relatedness <- table_Spearman_allEmbeddings[10:nrow(table_Spearman_allEmbeddings),]
table_Spearman_allEmbeddings_relatedness <- rbind(table_Spearman_allEmbeddings_relatedness, Avg_rho = colMeans(table_Spearman_allEmbeddings_relatedness[,1:ncol(table_Spearman_allEmbeddings_relatedness)]))

table_Harmonic_allEmbeddings_relatedness <- table_Harmonic_allEmbeddings[10:nrow(table_Harmonic_allEmbeddings),]
table_Harmonic_allEmbeddings_relatedness <- rbind(table_Harmonic_allEmbeddings_relatedness, Avg_h = colMeans(table_Harmonic_allEmbeddings_relatedness[,1:ncol(table_Harmonic_allEmbeddings_relatedness)]))

table_joined_allEmbeddings_relatedness <- rbind(table_Pearson_allEmbeddings_relatedness, table_Spearman_allEmbeddings_relatedness, table_Harmonic_allEmbeddings_relatedness)

# We transpose the matrices in order to sort them according to the
# average value obtained by each embedding model

table_joined_allEmbeddings_relatedness <- t(table_joined_allEmbeddings_relatedness)
table_joined_allEmbeddings_relatedness <- mat.sort(table_joined_allEmbeddings_relatedness, ncol(table_joined_allEmbeddings_relatedness), decreasing = TRUE)
table_joined_allEmbeddings_relatedness <- t(table_joined_allEmbeddings_relatedness)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_joined_allEmbeddings_similarity_rounded <- round(table_joined_allEmbeddings_similarity, 3)
table_joined_allEmbeddings_relatedness_rounded <- round(table_joined_allEmbeddings_relatedness, 3)

# We save all the final assembled data tables 

write.csv(table_joined_allEmbeddings_similarity, file = paste(outputDir, sep="","table_joined_allEmbeddings_similarity.csv"))
write.csv(table_joined_allEmbeddings_similarity_rounded, file = paste(outputDir, sep="","table_joined_allEmbeddings_similarity_rounded.csv"))

write.csv(table_joined_allEmbeddings_relatedness, file = paste(outputDir, sep="","table_joined_allEmbeddings_relatedness.csv"))
write.csv(table_joined_allEmbeddings_relatedness_rounded, file = paste(outputDir, sep="","table_joined_allEmbeddings_relatedness_rounded.csv"))

# ---------------------------------------------------------------------
# Table of p-values between Attract-reppel model and the remaining
# measures in following tables:
#
# (1) table_Pearson_SimDatasets.
# (2) table_Spearman_SimDatasets.
# (3) table_Harmonic_SimDatasets.
#
# This information is used to draw strong conclusions on the performance
# of Attract-reppel model.
# ---------------------------------------------------------------------

table_pvalues_AttractReppel_SimDatasets <- matrix(ncol = 6, nrow = nrow(table_Pearson_SimDatasets) - 1)

colnames(table_pvalues_AttractReppel_SimDatasets)<-c("Measure","p-value(r)","Measure","p-value(rho)","Measure","p-value(h)")

for (iMeasure in 1:nrow(table_pvalues_AttractReppel_SimDatasets))
{
  # We set the names of the measures evaluated
  
	table_pvalues_AttractReppel_SimDatasets[iMeasure, 1] <- rownames(table_Pearson_SimDatasets)[iMeasure + 1]
	table_pvalues_AttractReppel_SimDatasets[iMeasure, 3] <- rownames(table_Spearman_SimDatasets)[iMeasure + 1]
	table_pvalues_AttractReppel_SimDatasets[iMeasure, 5] <- rownames(table_Harmonic_SimDatasets)[iMeasure + 1]
	
	table_pvalues_AttractReppel_SimDatasets[iMeasure, 2] <- signif(t.test(table_Pearson_SimDatasets[1,1:5],
	                                                                      table_Pearson_SimDatasets[iMeasure + 1,1:5],
	                                                                      paired = TRUE,alternative="greater")$p.value,
	                                                               digits=2)
	
	
	table_pvalues_AttractReppel_SimDatasets[iMeasure, 4] <- signif(t.test(table_Spearman_SimDatasets[1,1:5],
	                                                                      table_Spearman_SimDatasets[iMeasure + 1,1:5],
	                                                                      paired = TRUE,alternative="greater")$p.value,
	                                                               digits=2)
	
	
	table_pvalues_AttractReppel_SimDatasets[iMeasure, 6] <- signif(t.test(table_Harmonic_SimDatasets[1,1:5],
	                                                                      table_Harmonic_SimDatasets[iMeasure + 1,1:5],
	                                                                      paired = TRUE,alternative="greater")$p.value,
	                                                               digits=2)
}

write.csv(table_pvalues_AttractReppel_SimDatasets, file = paste(outputDir, sep="","table_pvalues_AttractReppel_SimDataset_values.csv"))

# ---------------------------------------------------------------------
# Tables of average similarity for each word embedding pair.
# We compute the average raw similarity value reported by each
# word pair for all combinations of two word embeddings.
# ---------------------------------------------------------------------

# We create a list with datasets including only word embeddings. First nine datasets
# are similarity datasets, whilst the rest are relatedness datasets. However,
# datasets 1-5 and 10-13 contain ontology-absed measures and embeddings, thus
# we must extract the columns associated to word embeddings.

rawdata_onlyEmbeddings_datasets <- list()

for (iDataset in 1:length(rawdataAllDatasets))
{
	# We get the raw data of the next dataset

	rawdata <- rawdataAllDatasets[[iDataset]]

	# We extract the human judgements column and all columns with emebddings

	human_judgements<-rawdata[,1:2]
	
	if ((iDataset <= 5) || ((iDataset >= 10) && (iDataset <= 13)))
  {
    rawdata_onlyEmbeddings_datasets[[iDataset]]<-cbind(human_judgements, rawdata[,24:ncol(rawdata)])      
	}
  else
  {
    rawdata_onlyEmbeddings_datasets[[iDataset]]<-rawdataAllDatasets[[iDataset]]
  }
}

# We create the 3 matrices of results for the combinations of embedding models.
# We substract the first two columns containig word pairs and human judgements

nWordModels <- ncol(rawdata_onlyEmbeddings_datasets[[1]]) - 2

averaged_Pearson_results <- matrix(nrow = nrow(table_Pearson_allEmbeddings),
                                 ncol = choose(nWordModels, 2),
                                 dimnames = list(rownames(table_Pearson_allEmbeddings),
                                                 c(1:choose(nWordModels, 2))))

averaged_Spearman_results <- averaged_Pearson_results
averaged_Harmonic_results <- averaged_Pearson_results

# We define the suffixes used to name the output files

filenames<-rawdataAllDatasetNames

# We compute the raw average similarity values for all word embedding pairs and datasets

for (iDataset in 1:length(rawdata_onlyEmbeddings_datasets))
{
	# We get the rawdata of the next dataset

	rawdata <- rawdata_onlyEmbeddings_datasets[[iDataset]]

	# We get the number of embeddings

	nEmbeddings <- ncol(rawdata) - 2

	# We create the matrix containing all raw similarity values for all combinations

	raw_averaged_similarity_matrix <- matrix(nrow = nrow(rawdata),
	                                         ncol = 2 + ncol(averaged_Pearson_results))
	
	colnames(raw_averaged_similarity_matrix) <- c(1:ncol(raw_averaged_similarity_matrix))

	# We copy the first two columns containing the word pairs and human judgements

	raw_averaged_similarity_matrix[,1] <- rawdata[,1]
	colnames(raw_averaged_similarity_matrix)[1] <- colnames(rawdata)[1]
	
	raw_averaged_similarity_matrix[,2] <- rawdata[,2]
	colnames(raw_averaged_similarity_matrix)[2] <- colnames(rawdata)[2]

	# We compute all averaged similarity values by selecting all combinations
	# of two different word emebeddings

	iModelPair <- 1

	for (i in 1:(nEmbeddings - 1))
	{
		for (j in (i + 1):nEmbeddings)
		{
			# We compute the average raw similarity values of the pair
 
			averaged_sim <- 0.5 * (rawdata[, i  + 2] + rawdata[, j  + 2])
			
			raw_averaged_similarity_matrix[, iModelPair + 2] <- averaged_sim
			
			colnames(raw_averaged_similarity_matrix)[iModelPair + 2] <- paste(colnames(rawdata)[i + 2], sep="+", colnames(rawdata)[j + 2])

			# We compute the Pearson, Spearman and Harmonic score as regards human judgements

			pearson <- cor(rawdata[,2], averaged_sim, method = "pearson")
			spearman <- cor(rawdata[,2], averaged_sim, method = "spearman")
			
			averaged_Pearson_results[iDataset, iModelPair] <- pearson
			averaged_Spearman_results[iDataset, iModelPair] <- spearman
			averaged_Harmonic_results[iDataset, iModelPair] <- 2 * pearson * spearman / (pearson + spearman)			

			# We recover and set the name of the combined word embedding
			
			combinedMethodName <- colnames(raw_averaged_similarity_matrix)[iModelPair + 2]
			
			colnames(averaged_Pearson_results)[iModelPair] <- combinedMethodName
			colnames(averaged_Spearman_results)[iModelPair] <- combinedMethodName
			colnames(averaged_Harmonic_results)[iModelPair] <- combinedMethodName

			# We increment the counter of embedding pairs

			iModelPair <- iModelPair + 1
		}
	}

	# We save the averaged raw similarity values for the current dataset

	baseFilename <- paste(outputDir, sep="", filenames[iDataset])
	
	write.table(raw_averaged_similarity_matrix, file = paste(baseFilename, sep="","_averaged_raw_similarity_values.csv"), sep=";")
}

#-------------------------------------------------------------
# We order the collection of averaged embedding pair results
#-------------------------------------------------------------

averaged_Pearson_results <- rbind(averaged_Pearson_results, Avg = colMeans(averaged_Pearson_results[,1:ncol(averaged_Pearson_results)]))
averaged_Pearson_results <- t(averaged_Pearson_results)
averaged_Pearson_results <- mat.sort(averaged_Pearson_results, ncol(averaged_Pearson_results), decreasing = TRUE)
averaged_Pearson_results <- t(averaged_Pearson_results)

averaged_Spearman_results <- rbind(averaged_Spearman_results, Avg = colMeans(averaged_Spearman_results[,1:ncol(averaged_Spearman_results)]))
averaged_Spearman_results <- t(averaged_Spearman_results)
averaged_Spearman_results <- mat.sort(averaged_Spearman_results, ncol(averaged_Spearman_results), decreasing = TRUE)
averaged_Spearman_results <- t(averaged_Spearman_results)

averaged_Harmonic_results <- rbind(averaged_Harmonic_results, Avg = colMeans(averaged_Harmonic_results[,1:ncol(averaged_Harmonic_results)]))
averaged_Harmonic_results <- t(averaged_Harmonic_results)
averaged_Harmonic_results <- mat.sort(averaged_Harmonic_results, ncol(averaged_Harmonic_results), decreasing = TRUE)
averaged_Harmonic_results <- t(averaged_Harmonic_results)

# We save all computed metrics for all word embedding pairs

write.csv(averaged_Pearson_results, file = paste(outputDir, sep="","table_Pearson_averaged_results.csv"))
write.csv(averaged_Spearman_results, file = paste(outputDir, sep="","table_Spearman_averaged_results.csv"))
write.csv(averaged_Harmonic_results, file = paste(outputDir, sep="","table_Harmonic_averaged_results.csv"))

averaged_Pearson_results_rounded <- round(averaged_Pearson_results, 3)
averaged_Spearman_results_rounded <- round(averaged_Spearman_results, 3)
averaged_Harmonic_results_rounded <- round(averaged_Harmonic_results, 3)

write.csv(averaged_Pearson_results_rounded, file = paste(outputDir, sep="","table_Pearson_averaged_results_rounded.csv"))
write.csv(averaged_Spearman_results_rounded, file = paste(outputDir, sep="","table_Spearman_averaged_results_rounded.csv"))
write.csv(averaged_Harmonic_results_rounded, file = paste(outputDir, sep="","table_Harmonic_averaged_results_rounded.csv"))
