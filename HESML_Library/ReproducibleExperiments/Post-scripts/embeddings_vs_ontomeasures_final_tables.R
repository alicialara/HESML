# Description:
#
# This script loads a collection of word similarity benchmarks generated
# by HESML, ehich contain the raw similarity values for each word pair.
# Then, the script computes a collection of consolidated tables
# including the Pearson and Spearman correlation metrics together
# with the harmonic score of the # former ones.
#
# References:
# ----------

# We clear all session variables

rm(list = ls())

# IMPORTANT:configuration of the input/output directories
# We define below the input directory for the input raw results
# in CSV file format, and the output directory for the
# final assembled tables in CSV file format.
# You must change these values in order to
# point to the proper directories in your hard drive.
# We also define below the name of the input raw CSV files
# containing the experimental results.

# The input and output directories below must end with '/' in
# Unix-like format to be compatible with Windows
# or Linux-based R distributions.

inputDir = "D:/Versiones_Git_HESML/HESML/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper/RawOutputFiles/"
mainOutputDir = "D:/Versiones_Git_HESML/HESML/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper/ProcessedOutputFiles/"
outputDir = "D:/Versiones_Git_HESML/HESML/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper/ProcessedOutputFiles/MainConsolidatedTables/"
outputAvgMetricFilesDir = "D:/Versiones_Git_HESML/HESML/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper/ProcessedOutputFiles/AveragedConsolidatedTables/"

dir.create(mainOutputDir)
dir.create(outputDir)
dir.create(outputAvgMetricFilesDir)

# Input raw CSV files generated by the reproducible experiments detailed
# in the companion paper.

raw_MC28_file <- "raw_similarity_values_MC28_dataset.csv"
raw_RG65_file <- "raw_similarity_values_RG65_dataset.csv"
raw_PSfull_file <- "raw_similarity_values_PSfull_dataset.csv"
raw_Agirre201_file <- "raw_similarity_values_Agirre201_lowercase_dataset.csv"
raw_SimLex665_file <- "raw_similarity_values_SimLex665_dataset.csv"
raw_MTurk771_file <- "raw_similarity_values_MTurk771_dataset.csv"
raw_MTurk287_235_file  <- "raw_similarity_values_MTurk287-235_dataset.csv"
raw_WS353Rel_file <- "raw_similarity_values_WS353Rel_dataset.csv"
raw_Rel122_file <- "raw_similarity_values_Rel122_dataset.csv"
raw_WS353Full_file <- "raw_similarity_values_WS353Full_dataset.csv"
raw_SimLex111_file <- "raw_similarity_values_SimLex111_dataset.csv"
raw_SimLex222_file <- "raw_similarity_values_SimLex222_dataset.csv"
raw_SimLex999_file <- "raw_similarity_values_SimLex999_dataset.csv"
raw_SimVerb3500_file <- "raw_similarity_values_SimVerb3500_dataset.csv"
raw_MEN_file <- "raw_similarity_values_MEN_dataset.csv"
raw_YP130_file <- "raw_similarity_values_YP130_dataset.csv"
raw_RareWords2034_file <- "raw_similarity_values_RareWords2034_dataset.csv"
raw_RareWords1401_file <- "raw_similarity_values_RareWords1401_dataset.csv"
raw_SCWS1994_file <- "raw_similarity_values_SCWS1994_dataset.csv"

# We load the input raw results file

rawdata_MC28 <- read.csv(paste(inputDir, sep = "", raw_MC28_file),dec = ".", sep = ';')
rawdata_RG65 <- read.csv(paste(inputDir, sep = "", raw_RG65_file),dec = ".", sep = ';')
rawdata_PSfull <- read.csv(paste(inputDir, sep = "", raw_PSfull_file),dec = ".", sep = ';')
rawdata_Agirre201 <- read.csv(paste(inputDir, sep = "", raw_Agirre201_file),dec = ".", sep = ';')
rawdata_SimLex665 <- read.csv(paste(inputDir, sep = "", raw_SimLex665_file),dec = ".", sep = ';')
rawdata_MTurk771 <- read.csv(paste(inputDir, sep = "", raw_MTurk771_file),dec = ".", sep = ';')
rawdata_MTurk287_235 <- read.csv(paste(inputDir, sep = "", raw_MTurk287_235_file),dec = ".", sep = ';')
rawdata_WS353Rel <- read.csv(paste(inputDir, sep = "", raw_WS353Rel_file),dec = ".", sep = ';')
rawdata_Rel122 <- read.csv(paste(inputDir, sep = "", raw_Rel122_file),dec = ".", sep = ';')
rawdata_WS353Full <- read.csv(paste(inputDir, sep = "", raw_WS353Full_file),dec = ".", sep = ';')
rawdata_SimLex111 <- read.csv(paste(inputDir, sep = "", raw_SimLex111_file),dec = ".", sep = ';')
rawdata_SimLex222 <- read.csv(paste(inputDir, sep = "", raw_SimLex222_file),dec = ".", sep = ';')
rawdata_SimLex999 <- read.csv(paste(inputDir, sep = "", raw_SimLex999_file),dec = ".", sep = ';')
rawdata_SimVerb3500 <- read.csv(paste(inputDir, sep = "", raw_SimVerb3500_file),dec = ".", sep = ';')
rawdata_MEN <- read.csv(paste(inputDir, sep = "", raw_MEN_file),dec = ".", sep = ';')
rawdata_YP130 <- read.csv(paste(inputDir, sep = "", raw_YP130_file),dec = ".", sep = ';')
rawdata_RareWords2034 <- read.csv(paste(inputDir, sep = "", raw_RareWords2034_file),dec = ".", sep = ';')
rawdata_RareWords1401 <- read.csv(paste(inputDir, sep = "", raw_RareWords1401_file),dec = ".", sep = ';')
rawdata_SCWS1994 <- read.csv(paste(inputDir, sep = "", raw_SCWS1994_file),dec = ".", sep = ';')

# IMPORTANT: you must install the 'BioPhysConnectoR' package before to run the next three lines of code
# We sort the tables 3,4 and 5 in descending order by using the column-based Average values (last row)

library(BioPhysConnectoR)

# ---------------------------------------------------------------------
# Raw output file format:
# Raw similarity files contain the similarity values returned by each
# semantic measure for each word pair. First nine dataset were evaluated
# with a set of 21 ontology-based measures and 11 word embedding models,
# whilst remaining datasets were only evaluated with the word embeddings.
# First two columns contain the word pairs and the human judgements,
# whilst subsequent columns contain the values returned by the
# twenty-one ontology-based measures and the eleven word embedding models.
# ---------------------------------------------------------------------

# ---------------------------------------------------------------------
# Tables 1,2 and 3: Pearson, Spearman and Harmonic mean metrics of all measures
# and embeddings in the 5 similarity datasets evaluated both by the
# ontology-based measures based on WordNet as the pre-trained word
# embedding models.
# ---------------------------------------------------------------------

# We define all datasets represented in table 1

rawdataSimNounDatasets <- list(rawdata_MC28, rawdata_RG65, rawdata_PSfull, rawdata_Agirre201, rawdata_SimLex665)

# We create a separated table for each metric by removing first two columns
# which contain the word pairs and human judgements. 
# Measures are aranged in rows whilst datasets are arranged in columns.

table_Pearson_SimDatasets <- matrix(nrow = ncol(rawdata_MC28) - 2,
                                  ncol = length(rawdataSimNounDatasets),
                                  dimnames = list(colnames(rawdata_MC28)[3:ncol(rawdata_MC28)],
                                                  c("MC28", "RG65", "PSfull", "Agirre201", "SimLex665")))

table_Spearman_SimDatasets <- table_Pearson_SimDatasets
table_Harmonic_SimDatasets <- table_Pearson_SimDatasets

# Loop for the computation of the metrics

nMeasures <- nrow(table_Pearson_SimDatasets)
nDatasets <- length(rawdataSimNounDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata <- rawdataSimNounDatasets[[iDataset]]

	# We evaluate the Pearson, Spearman and Harmonic metrics for each measure in the current dataset

	for (iMeasure in 1:nMeasures)
	{
		table_Pearson_SimDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "pearson")
		table_Spearman_SimDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "spearman")
		table_Harmonic_SimDatasets[iMeasure, iDataset] <- 2.0 * table_Pearson_SimDatasets[iMeasure, iDataset] * table_Spearman_SimDatasets[iMeasure, iDataset] / (table_Pearson_SimDatasets[iMeasure, iDataset] + table_Spearman_SimDatasets[iMeasure, iDataset])
	}
}

# We compute the average values per row and sort the rows

table_Pearson_SimDatasets <- cbind(table_Pearson_SimDatasets, Avg = rowMeans(table_Pearson_SimDatasets[1:nrow(table_Pearson_SimDatasets),]))
table_Pearson_SimDatasets <- mat.sort(table_Pearson_SimDatasets, ncol(table_Pearson_SimDatasets), decreasing = TRUE)

table_Spearman_SimDatasets <- cbind(table_Spearman_SimDatasets, Avg = rowMeans(table_Spearman_SimDatasets[1:nrow(table_Spearman_SimDatasets),]))
table_Spearman_SimDatasets <- mat.sort(table_Spearman_SimDatasets, ncol(table_Spearman_SimDatasets), decreasing = TRUE)

table_Harmonic_SimDatasets <- cbind(table_Harmonic_SimDatasets, Avg = rowMeans(table_Harmonic_SimDatasets[1:nrow(table_Harmonic_SimDatasets),]))
table_Harmonic_SimDatasets <- mat.sort(table_Harmonic_SimDatasets, ncol(table_Harmonic_SimDatasets), decreasing = TRUE)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_Pearson_SimDatasets_rounded <- round(table_Pearson_SimDatasets, 3);
table_Spearman_SimDatasets_rounded <- round(table_Spearman_SimDatasets, 3);
table_Harmonic_SimDatasets_rounded <- round(table_Harmonic_SimDatasets, 3);

# We save all final assembled data tables 

write.csv(table_Pearson_SimDatasets, file = paste(outputDir, sep="","table_Pearson_SimDatasets.csv"))
write.csv(table_Pearson_SimDatasets_rounded, file = paste(outputDir, sep="","table_Pearson_SimDatasets_rounded.csv"))

write.csv(table_Spearman_SimDatasets, file = paste(outputDir, sep="","table_Spearman_SimDatasets.csv"))
write.csv(table_Spearman_SimDatasets_rounded, file = paste(outputDir, sep="","table_Spearman_SimDatasets_rounded.csv"))

write.csv(table_Harmonic_SimDatasets, file = paste(outputDir, sep="","table_Harmonic_SimDatasets.csv"))
write.csv(table_Harmonic_SimDatasets_rounded, file = paste(outputDir, sep="","table_Harmonic_SimDatasets_rounded.csv"))

# ---------------------------------------------------------------------
# Table 4,5 and 6: Pearson, Spearman and Harmonic mean metrics of all measures
# and embeddings in the 4 relatedness datasets evaluated both by the
# ontoloy-based measures basedon Wordnet as the pre-trained word
# embedding models.
# ---------------------------------------------------------------------

# We define all relatedness datasets represented in tables

rawdataRelNounDatasets <- list(rawdata_MTurk771, rawdata_MTurk287_235, rawdata_WS353Rel, rawdata_Rel122)

# We create the table 2

# We create a separated table for each metric

table_Pearson_RelDatasets <- matrix(nrow = ncol(rawdata_MC28) - 2,
                                    ncol = length(rawdataRelNounDatasets),
                                    dimnames = list(colnames(rawdata_MC28)[3:ncol(rawdata_MC28)],
                                                    c("MTurk771", "MTurk287_235", "WS353Rel", "Rel122")))

table_Spearman_RelDatasets <- table_Pearson_RelDatasets
table_Harmonic_RelDatasets <- table_Pearson_RelDatasets

# Loop for the computation of all metrics

nMeasures <- nrow(table_Pearson_RelDatasets)
nDatasets <- length(rawdataRelNounDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata <- rawdataRelNounDatasets[[iDataset]]

	# We evaluate the Pearson, Spearman and Harmonic metrics for each measure in the current dataset

	for (iMeasure in 1:nMeasures)
	{
		table_Pearson_RelDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "pearson")
		table_Spearman_RelDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "spearman")
		table_Harmonic_RelDatasets[iMeasure, iDataset] <- 2.0 * table_Pearson_RelDatasets[iMeasure, iDataset] * table_Spearman_RelDatasets[iMeasure, iDataset] / (table_Pearson_RelDatasets[iMeasure, iDataset] + table_Spearman_RelDatasets[iMeasure, iDataset])
	}
}

# We compute the average values per row and sort the rows

table_Pearson_RelDatasets <- cbind(table_Pearson_RelDatasets, Avg = rowMeans(table_Pearson_RelDatasets[1:nrow(table_Pearson_RelDatasets),]))
table_Pearson_RelDatasets <- mat.sort(table_Pearson_RelDatasets, ncol(table_Pearson_RelDatasets), decreasing = TRUE)

table_Spearman_RelDatasets <- cbind(table_Spearman_RelDatasets, Avg = rowMeans(table_Spearman_RelDatasets[1:nrow(table_Spearman_RelDatasets),]))
table_Spearman_RelDatasets <- mat.sort(table_Spearman_RelDatasets, ncol(table_Spearman_RelDatasets), decreasing = TRUE)

table_Harmonic_RelDatasets <- cbind(table_Harmonic_RelDatasets, Avg = rowMeans(table_Harmonic_RelDatasets[1:nrow(table_Harmonic_RelDatasets),]))
table_Harmonic_RelDatasets <- mat.sort(table_Harmonic_RelDatasets, ncol(table_Harmonic_RelDatasets), decreasing = TRUE)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_Pearson_RelDatasets_rounded <- round(table_Pearson_RelDatasets, 3);
table_Spearman_RelDatasets_rounded <- round(table_Spearman_RelDatasets, 3);
table_Harmonic_RelDatasets_rounded <- round(table_Harmonic_RelDatasets, 3);

# We save all final assembled data tables 

write.csv(table_Pearson_RelDatasets, file = paste(outputDir, sep="","table_Pearson_RelDatasets.csv"))
write.csv(table_Pearson_RelDatasets_rounded, file = paste(outputDir, sep="","table_Pearson_RelDatasets_rounded.csv"))

write.csv(table_Spearman_RelDatasets, file = paste(outputDir, sep="","table_Spearman_RelDatasets.csv"))
write.csv(table_Spearman_RelDatasets_rounded, file = paste(outputDir, sep="","table_Spearman_RelDatasets_rounded.csv"))

write.csv(table_Harmonic_RelDatasets, file = paste(outputDir, sep="","table_Harmonic_RelDatasets.csv"))
write.csv(table_Harmonic_RelDatasets_rounded, file = paste(outputDir, sep="","table_Harmonic_RelDatasets_rounded.csv"))

# ---------------------------------------------------------------------
# Table 7, 8 and 9: Pearson, Spearman and Harmonic metrics of all
# pre-trained embeddings models in all datasets. Rows contain datasets
# whilst columns contain the perfromance of each word embedding model
# in each dataset. Columns are sorted in descending order from
# left to right. Leftmost columns show best performing embedding models.
# ---------------------------------------------------------------------

# We define all datasets

rawdataAllDatasets<-list(rawdata_MC28, rawdata_RG65, rawdata_PSfull, rawdata_Agirre201, rawdata_SimLex665, rawdata_SimLex111, rawdata_SimLex222, rawdata_SimLex999, rawdata_SimVerb3500, rawdata_MTurk771, rawdata_MTurk287_235, rawdata_WS353Rel, rawdata_Rel122, rawdata_WS353Full, rawdata_MEN, rawdata_YP130, rawdata_RareWords2034, rawdata_RareWords1401, rawdata_SCWS1994);
rawdataAllDatasetNames<-c("MC28", "RG65", "PSfull", "Agirre201", "SimLex665", "SimLex111", "SimLex222", "SimLex999", "SimVerb3500", "MTurk771", "MTurk287_235", "WS353Rel", "Rel122", "WS353Full", "MEN", "YP130", "RW2034", "RW1401", "SCWS1994")

# We create the tables 3,4 and 5

table_Pearson_allEmbeddings <- matrix(ncol = 11, nrow = length(rawdataAllDatasets),
                                      dimnames = list(rawdataAllDatasetNames,
                                                      colnames(rawdata_WS353Full)[3:ncol(rawdata_WS353Full)]))

table_Spearman_allEmbeddings <- table_Pearson_allEmbeddings
table_Harmonic_allEmbeddings <- table_Pearson_allEmbeddings

nOntologyBasedMeasures = 21;
nEmbeddings <- ncol(table_Pearson_allEmbeddings)
nDatasets <- length(rawdataAllDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata <- rawdataAllDatasets[[iDataset]]

	# We define the offset position to extract the column of each embedding model.
	# First nine raw input datasets include the evaluation of 21 ontology-based measures and
	# 11 embedding models. However, the remaining 10 datasets are only evaluated
	# on the embedding models.

	if ((iDataset <= 5) || ((iDataset >= 10) && (iDataset <= 13)))
	{
		iOffset <- nOntologyBasedMeasures + 2;
	}
	else
	{
		iOffset <- 2;
	}

	# We evaluate the Pearson, Spearman and Harmonic metrics for each
	# embeding model in the current dataset

	for (iEmbedding in 1:nEmbeddings)
	{
		table_Pearson_allEmbeddings[iDataset, iEmbedding] <- cor(rawdata[,2], rawdata[, iEmbedding + iOffset], method = "pearson")
		table_Spearman_allEmbeddings[iDataset, iEmbedding] <- cor(rawdata[,2], rawdata[, iEmbedding + iOffset], method = "spearman")
		table_Harmonic_allEmbeddings[iDataset, iEmbedding] <- 2 * table_Pearson_allEmbeddings[iDataset, iEmbedding] * table_Spearman_allEmbeddings[iDataset, iEmbedding] / (table_Pearson_allEmbeddings[iDataset, iEmbedding] + table_Spearman_allEmbeddings[iDataset, iEmbedding])
	}
}

#-----------------------------------------------------------------
# PERFORMANCE of word embeddings only in SIMILARITY DATASETS
# We extract the performance of all models in all similarity datasets
#-----------------------------------------------------------------

# We extract the similarity datasets

table_Pearson_allEmbeddings_similarity <- table_Pearson_allEmbeddings[1:9,]
table_Pearson_allEmbeddings_similarity <- rbind(table_Pearson_allEmbeddings_similarity, Avg_r = colMeans(table_Pearson_allEmbeddings_similarity[,1:ncol(table_Pearson_allEmbeddings_similarity)]))

table_Spearman_allEmbeddings_similarity <- table_Spearman_allEmbeddings[1:9,]
table_Spearman_allEmbeddings_similarity <- rbind(table_Spearman_allEmbeddings_similarity, Avg_rho = colMeans(table_Spearman_allEmbeddings_similarity[,1:ncol(table_Spearman_allEmbeddings_similarity)]))

table_Harmonic_allEmbeddings_similarity <- table_Harmonic_allEmbeddings[1:9,]
table_Harmonic_allEmbeddings_similarity <- rbind(table_Harmonic_allEmbeddings_similarity, Avg_h = colMeans(table_Harmonic_allEmbeddings_similarity[,1:ncol(table_Harmonic_allEmbeddings_similarity)]))

table_joined_allEmbeddings_similarity = rbind(table_Pearson_allEmbeddings_similarity, table_Spearman_allEmbeddings_similarity, table_Harmonic_allEmbeddings_similarity)

# We transpose the matrices in order to sort them according to the
# average value obtained by each embedding model

table_joined_allEmbeddings_similarity <- t(table_joined_allEmbeddings_similarity)
table_joined_allEmbeddings_similarity <- mat.sort(table_joined_allEmbeddings_similarity, ncol(table_joined_allEmbeddings_similarity), decreasing = TRUE)
table_joined_allEmbeddings_similarity <- t(table_joined_allEmbeddings_similarity)

#-----------------------------------------------------------------
# PERFORMANCE of word embeddings only in RELATEDNESS DATASETS
# We extract the performance of all models in all similarity datasets
#-----------------------------------------------------------------

table_Pearson_allEmbeddings_relatedness <- table_Pearson_allEmbeddings[10:nrow(table_Pearson_allEmbeddings),]
table_Pearson_allEmbeddings_relatedness <- rbind(table_Pearson_allEmbeddings_relatedness, Avg_r = colMeans(table_Pearson_allEmbeddings_relatedness[,1:ncol(table_Pearson_allEmbeddings_relatedness)]))

table_Spearman_allEmbeddings_relatedness <- table_Spearman_allEmbeddings[10:nrow(table_Spearman_allEmbeddings),]
table_Spearman_allEmbeddings_relatedness <- rbind(table_Spearman_allEmbeddings_relatedness, Avg_rho = colMeans(table_Spearman_allEmbeddings_relatedness[,1:ncol(table_Spearman_allEmbeddings_relatedness)]))

table_Harmonic_allEmbeddings_relatedness <- table_Harmonic_allEmbeddings[10:nrow(table_Harmonic_allEmbeddings),]
table_Harmonic_allEmbeddings_relatedness <- rbind(table_Harmonic_allEmbeddings_relatedness, Avg_h = colMeans(table_Harmonic_allEmbeddings_relatedness[,1:ncol(table_Harmonic_allEmbeddings_relatedness)]))

table_joined_allEmbeddings_relatedness <- rbind(table_Pearson_allEmbeddings_relatedness, table_Spearman_allEmbeddings_relatedness, table_Harmonic_allEmbeddings_relatedness)

# We transpose the matrices in order to sort them according to the
# average value obtained by each embedding model

table_joined_allEmbeddings_relatedness <- t(table_joined_allEmbeddings_relatedness)
table_joined_allEmbeddings_relatedness <- mat.sort(table_joined_allEmbeddings_relatedness, ncol(table_joined_allEmbeddings_relatedness), decreasing = TRUE)
table_joined_allEmbeddings_relatedness <- t(table_joined_allEmbeddings_relatedness)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_joined_allEmbeddings_similarity_rounded <- round(table_joined_allEmbeddings_similarity, 3)
table_joined_allEmbeddings_relatedness_rounded <- round(table_joined_allEmbeddings_relatedness, 3)

# We save all the final assembled data tables 

write.csv(table_joined_allEmbeddings_similarity, file = paste(outputDir, sep="","table_joined_allEmbeddings_similarity.csv"))
write.csv(table_joined_allEmbeddings_similarity_rounded, file = paste(outputDir, sep="","table_joined_allEmbeddings_similarity_rounded.csv"))

write.csv(table_joined_allEmbeddings_relatedness, file = paste(outputDir, sep="","table_joined_allEmbeddings_relatedness.csv"))
write.csv(table_joined_allEmbeddings_relatedness_rounded, file = paste(outputDir, sep="","table_joined_allEmbeddings_relatedness_rounded.csv"))

# ---------------------------------------------------------------------
# Table of p-values between Attract-reppel model and the remaining
# measures in following tables:
#
# (1) table_Pearson_SimDatasets.
# (2) table_Spearman_SimDatasets.
# (3) table_Harmonic_SimDatasets.
#
# This information is used to draw strong conclusions on the performance
# of Attract-reppel model.
# ---------------------------------------------------------------------

# We initialize the p-valu vectors incuding their names

pvalues_r <- matrix(ncol = 1, nrow = nrow(table_Pearson_SimDatasets) - 1)
pvalues_rho <- matrix(ncol = 1, nrow = nrow(table_Pearson_SimDatasets) - 1)
pvalues_harmonic <- matrix(ncol = 1, nrow = nrow(table_Pearson_SimDatasets) - 1)

rownames(pvalues_r) <- c(1:nrow(pvalues_r))
rownames(pvalues_rho) <- c(1:nrow(pvalues_rho))
rownames(pvalues_harmonic) <- c(1:nrow(pvalues_harmonic))

for (iMeasure in 1:nrow(pvalues_r))
{
  # We set the names of the measures evaluated
  
	rownames(pvalues_r)[iMeasure] <- rownames(table_Pearson_SimDatasets)[iMeasure + 1]
	rownames(pvalues_rho)[iMeasure] <- rownames(table_Spearman_SimDatasets)[iMeasure + 1]
	rownames(pvalues_harmonic)[iMeasure] <- rownames(table_Harmonic_SimDatasets)[iMeasure + 1]
	
	# We set the p-values
	
	pvalues_r[iMeasure] <- signif(t.test(table_Pearson_SimDatasets[1,1:5],
	                                        table_Pearson_SimDatasets[iMeasure + 1,1:5],
	                                        paired = TRUE,alternative="greater")$p.value,
	                                 digits=2)
	
	
	pvalues_rho[iMeasure] <- signif(t.test(table_Spearman_SimDatasets[1,1:5],
	                                          table_Spearman_SimDatasets[iMeasure + 1,1:5],
	                                          paired = TRUE,alternative="greater")$p.value,
	                                   digits=2)
	
	
	pvalues_harmonic[iMeasure] <- signif(t.test(table_Harmonic_SimDatasets[1,1:5],
	                                            table_Harmonic_SimDatasets[iMeasure + 1,1:5],
	                                            paired = TRUE,alternative="greater")$p.value,
	                                     digits=2)
}

# We sort the p-values in descending order

pvalues_r <- mat.sort(pvalues_r, 1, decreasing = TRUE)
pvalues_rho <- mat.sort(pvalues_rho, ncol(pvalues_rho), decreasing = TRUE)
pvalues_harmonic <- mat.sort(pvalues_harmonic, ncol(pvalues_harmonic), decreasing = TRUE)

# We group p-values in a matrix

table_pvalues_AttractReppel_nounSimDatasets <- matrix(ncol = 6, nrow = nrow(table_Pearson_SimDatasets) - 1)

colnames(table_pvalues_AttractReppel_nounSimDatasets)<-c("Measure","p-value(r)","Measure","p-value(rho)","Measure","p-value(h)")

table_pvalues_AttractReppel_nounSimDatasets[,1] <- names(pvalues_r)
table_pvalues_AttractReppel_nounSimDatasets[,2] <- pvalues_r
table_pvalues_AttractReppel_nounSimDatasets[,3] <- names(pvalues_rho)
table_pvalues_AttractReppel_nounSimDatasets[,4] <- pvalues_rho
table_pvalues_AttractReppel_nounSimDatasets[,5] <- names(pvalues_harmonic)
table_pvalues_AttractReppel_nounSimDatasets[,6] <- pvalues_harmonic

write.csv(table_pvalues_AttractReppel_nounSimDatasets, file = paste(outputDir, sep="","table_pvalues_AttractReppel_nounSimDataset_values.csv"))

# ---------------------------------------------------------------------
# Table of p-values between Paragram-ws model and the remaining
# measures in following tables:
#
# (1) table_Pearson_RelatednessDatasets.
# (2) table_Spearman_RelatednessDatasets.
# (3) table_Harmonic_RelatednessDatasets.
#
# This information is used to draw strong conclusions on the performance
# of Paragram-ws model.
# ---------------------------------------------------------------------

# We initialize the p-valu vectors incuding their names

pvalues_r <- matrix(ncol = 1, nrow = nrow(table_Pearson_RelDatasets) - 1)
pvalues_rho <- matrix(ncol = 1, nrow = nrow(table_Spearman_RelDatasets) - 1)
pvalues_harmonic <- matrix(ncol = 1, nrow = nrow(table_Harmonic_RelDatasets) - 1)

rownames(pvalues_r) <- c(1:nrow(pvalues_r))
rownames(pvalues_rho) <- c(1:nrow(pvalues_rho))
rownames(pvalues_harmonic) <- c(1:nrow(pvalues_harmonic))

# We need to skip second row for Pearson correlation because Paragram-ws
# obtains the second average Pearson value insted of being the first one.

iPearsonTargetMeasures <- c(1, 3:nrow(table_Pearson_RelDatasets))

for (iMeasure in 1:nrow(pvalues_r))
{
  # We set the index of the current compared measure
  
  iTargetMeasure <- iMeasure + 1
  
  # We set the names of the measures evaluated
  
  rownames(pvalues_r)[iMeasure] <- rownames(table_Pearson_RelDatasets)[iPearsonTargetMeasures[iMeasure]]
  rownames(pvalues_rho)[iMeasure] <- rownames(table_Spearman_RelDatasets)[iTargetMeasure]
  rownames(pvalues_harmonic)[iMeasure] <- rownames(table_Harmonic_RelDatasets)[iTargetMeasure]
  
  # We set the p-values
  
  pvalues_r[iMeasure] <- signif(t.test(table_Pearson_RelDatasets[2,1:4],
                                       table_Pearson_RelDatasets[iPearsonTargetMeasures[iMeasure],1:4],
                                       paired = TRUE,alternative="greater")$p.value,
                                digits=2)
  
  
  pvalues_rho[iMeasure] <- signif(t.test(table_Spearman_RelDatasets[1,1:4],
                                         table_Spearman_RelDatasets[iTargetMeasure,1:4],
                                         paired = TRUE,alternative="greater")$p.value,
                                  digits=2)
  
  
  pvalues_harmonic[iMeasure] <- signif(t.test(table_Harmonic_RelDatasets[1,1:4],
                                              table_Harmonic_RelDatasets[iTargetMeasure,1:4],
                                              paired = TRUE,alternative="greater")$p.value,
                                       digits=2)
}

# We sort the p-values in descending order

pvalues_r <- mat.sort(pvalues_r, 1, decreasing = TRUE)
pvalues_rho <- mat.sort(pvalues_rho, ncol(pvalues_rho), decreasing = TRUE)
pvalues_harmonic <- mat.sort(pvalues_harmonic, ncol(pvalues_harmonic), decreasing = TRUE)

# We group p-values in a matrix

table_pvalues_Paragramws_nounRelatednessDatasets <- matrix(ncol = 6, nrow = nrow(table_Pearson_RelDatasets) - 1)

colnames(table_pvalues_Paragramws_nounRelatednessDatasets)<-c("Measure","p-value(r)","Measure","p-value(rho)","Measure","p-value(h)")

table_pvalues_Paragramws_nounRelatednessDatasets[,1] <- names(pvalues_r)
table_pvalues_Paragramws_nounRelatednessDatasets[,2] <- pvalues_r
table_pvalues_Paragramws_nounRelatednessDatasets[,3] <- names(pvalues_rho)
table_pvalues_Paragramws_nounRelatednessDatasets[,4] <- pvalues_rho
table_pvalues_Paragramws_nounRelatednessDatasets[,5] <- names(pvalues_harmonic)
table_pvalues_Paragramws_nounRelatednessDatasets[,6] <- pvalues_harmonic

write.csv(table_pvalues_Paragramws_nounRelatednessDatasets, file = paste(outputDir, sep="","table_pvalues_Paragramws_nounRelatednessDatasets_values.csv"))

# ---------------------------------------------------------------------
# Table of p-values between Attract-reppel model and the remaining
# word embedding models in following tables:
#
# (1) table_Pearson_allEmbeddings_similarity
# (2) table_Spearman_allEmbeddings_similarity
# (3) table_Harmonic_allEmbeddings_similarity
#
# This information is used to draw strong conclusions on the performance
# of Attract-reppel model.
# ---------------------------------------------------------------------

# We initialize the p-value vectors incuding their names

pvalues_r <- matrix(ncol = 1, nrow = ncol(table_Pearson_allEmbeddings_similarity) - 1)
pvalues_rho <- matrix(ncol = 1, nrow = ncol(table_Spearman_allEmbeddings_similarity) - 1)
pvalues_harmonic <- matrix(ncol = 1, nrow = ncol(table_Harmonic_allEmbeddings_similarity) - 1)

rownames(pvalues_r) <- c(1:nrow(pvalues_r))
rownames(pvalues_rho) <- c(1:nrow(pvalues_rho))
rownames(pvalues_harmonic) <- c(1:nrow(pvalues_harmonic))

for (iMeasure in 1:nrow(pvalues_r))
{
  # We set the names of the measures evaluated
  
  rownames(pvalues_r)[iMeasure] <- colnames(table_Pearson_allEmbeddings_similarity)[iMeasure + 1]
  rownames(pvalues_rho)[iMeasure] <- colnames(table_Spearman_allEmbeddings_similarity)[iMeasure + 1]
  rownames(pvalues_harmonic)[iMeasure] <- colnames(table_Harmonic_allEmbeddings_similarity)[iMeasure + 1]
  
  # We set the p-values
  
  pvalues_r[iMeasure] <- signif(t.test(table_Pearson_allEmbeddings_similarity[1:9, 1],
                                       table_Pearson_allEmbeddings_similarity[1:9, iMeasure + 1],
                                       paired = TRUE,alternative="greater")$p.value,
                                digits=2)
  
  
  pvalues_rho[iMeasure] <- signif(t.test(table_Spearman_allEmbeddings_similarity[1:9, 1],
                                         table_Spearman_allEmbeddings_similarity[1:9, iMeasure + 1],
                                         paired = TRUE,alternative="greater")$p.value,
                                  digits=2)
  
  
  pvalues_harmonic[iMeasure] <- signif(t.test(table_Harmonic_allEmbeddings_similarity[1:9, 1],
                                              table_Harmonic_allEmbeddings_similarity[1:9, iMeasure + 1],
                                              paired = TRUE,alternative="greater")$p.value,
                                       digits=2)
}

# We sort the p-values in descending order

pvalues_r <- mat.sort(pvalues_r, 1, decreasing = TRUE)
pvalues_rho <- mat.sort(pvalues_rho, ncol(pvalues_rho), decreasing = TRUE)
pvalues_harmonic <- mat.sort(pvalues_harmonic, ncol(pvalues_harmonic), decreasing = TRUE)

# We group p-values in a matrix

table_pvalues_AttractReppel_allembeddings_similarity <- matrix(ncol = 6, nrow = ncol(table_Pearson_allEmbeddings_similarity) - 1)

colnames(table_pvalues_AttractReppel_allembeddings_similarity)<-c("Measure","p-value(r)","Measure","p-value(rho)","Measure","p-value(h)")

table_pvalues_AttractReppel_allembeddings_similarity[,1] <- names(pvalues_r)
table_pvalues_AttractReppel_allembeddings_similarity[,2] <- pvalues_r
table_pvalues_AttractReppel_allembeddings_similarity[,3] <- names(pvalues_rho)
table_pvalues_AttractReppel_allembeddings_similarity[,4] <- pvalues_rho
table_pvalues_AttractReppel_allembeddings_similarity[,5] <- names(pvalues_harmonic)
table_pvalues_AttractReppel_allembeddings_similarity[,6] <- pvalues_harmonic

write.csv(table_pvalues_AttractReppel_allembeddings_similarity, file = paste(outputDir, sep="","table_pvalues_AttractReppel_allembeddings_similarity.csv"))

# ---------------------------------------------------------------------
# Table of p-values between Paragra,-ws model and the remaining
# word embedding models in following tables:
#
# (1) table_Pearson_allEmbeddings_relatedness
# (2) table_Spearman_allEmbeddings__relatedness
# (3) table_Harmonic_allEmbeddings__relatedness
#
# This information is used to draw strong conclusions on the performance
# of Attract-reppel model.
# ---------------------------------------------------------------------

# We initialize the p-value vectors incuding their names

pvalues_r <- matrix(ncol = 1, nrow = ncol(table_Pearson_allEmbeddings_relatedness) - 1)
pvalues_rho <- matrix(ncol = 1, nrow = ncol(table_Spearman_allEmbeddings_relatedness) - 1)
pvalues_harmonic <- matrix(ncol = 1, nrow = ncol(table_Harmonic_allEmbeddings_relatedness) - 1)

rownames(pvalues_r) <- c(1:nrow(pvalues_r))
rownames(pvalues_rho) <- c(1:nrow(pvalues_rho))
rownames(pvalues_harmonic) <- c(1:nrow(pvalues_harmonic))

indexMeasures <-c(1:5,7:11)

for (iMeasure in 1:nrow(pvalues_r))
{
  # We set the names of the measures evaluated
  
  rownames(pvalues_r)[iMeasure] <- colnames(table_Pearson_allEmbeddings_relatedness)[indexMeasures[iMeasure]]
  rownames(pvalues_rho)[iMeasure] <- colnames(table_Spearman_allEmbeddings_relatedness)[indexMeasures[iMeasure]]
  rownames(pvalues_harmonic)[iMeasure] <- colnames(table_Harmonic_allEmbeddings_relatedness)[indexMeasures[iMeasure]]
  
  # We set the p-values. Paragram-ws is in the sixth column.
  
  pvalues_r[iMeasure] <- signif(t.test(table_Pearson_allEmbeddings_relatedness[1:10, 6],
                                       table_Pearson_allEmbeddings_relatedness[1:10, indexMeasures[iMeasure]],
                                       paired = TRUE,alternative="greater")$p.value,
                                digits=2)
  
  
  pvalues_rho[iMeasure] <- signif(t.test(table_Spearman_allEmbeddings_relatedness[1:10, 6],
                                         table_Spearman_allEmbeddings_relatedness[1:10, indexMeasures[iMeasure]],
                                         paired = TRUE,alternative="greater")$p.value,
                                  digits=2)
  
  
  pvalues_harmonic[iMeasure] <- signif(t.test(table_Harmonic_allEmbeddings_relatedness[1:10, 6],
                                              table_Harmonic_allEmbeddings_relatedness[1:10, indexMeasures[iMeasure]],
                                              paired = TRUE,alternative="greater")$p.value,
                                       digits=2)
}

# We sort the p-values in descending order

pvalues_r <- mat.sort(pvalues_r, 1, decreasing = TRUE)
pvalues_rho <- mat.sort(pvalues_rho, ncol(pvalues_rho), decreasing = TRUE)
pvalues_harmonic <- mat.sort(pvalues_harmonic, ncol(pvalues_harmonic), decreasing = TRUE)

# We group p-values in a matrix

table_pvalues_Paragramws_allembeddings_relatedness <- matrix(ncol = 6, nrow = ncol(table_Pearson_allEmbeddings_relatedness) - 1)

colnames(table_pvalues_Paragramws_allembeddings_relatedness)<-c("Measure","p-value(r)","Measure","p-value(rho)","Measure","p-value(h)")

table_pvalues_Paragramws_allembeddings_relatedness[,1] <- names(pvalues_r)
table_pvalues_Paragramws_allembeddings_relatedness[,2] <- pvalues_r
table_pvalues_Paragramws_allembeddings_relatedness[,3] <- names(pvalues_rho)
table_pvalues_Paragramws_allembeddings_relatedness[,4] <- pvalues_rho
table_pvalues_Paragramws_allembeddings_relatedness[,5] <- names(pvalues_harmonic)
table_pvalues_Paragramws_allembeddings_relatedness[,6] <- pvalues_harmonic

write.csv(table_pvalues_Paragramws_allembeddings_relatedness, file = paste(outputDir, sep="","table_pvalues_Paragramws_allembeddings_relatedness.csv"))

# ---------------------------------------------------------------------
# COMPUTATION OF AVERAGED MODELS OF SIMILARITY
#
# This section computes the semantic similarity resulting from
# the averaging of the best similarity measure in the noun similarity
# datasets with each other measure, both WE and OM measures.
# ---------------------------------------------------------------------

#  We create a table for Spearman metric by removing first two columns
# which contain the word pairs and human judgements. 
# Measures are arranged in rows whilst datasets are arranged in columns.
#  This table reproduces the same table reporting the Spearman correlation
# values of all measure in the five noun similarity datasets but using
# the averaged measures.
#  Input data is the vectors of raw similarity values reported by each
# measure.

table_AvgMeasures_Spearman_SimDatasets <- matrix(nrow = ncol(rawdata_MC28) - 2,
                                    ncol = length(rawdataSimNounDatasets),
                                    dimnames = list(colnames(rawdata_MC28)[3:ncol(rawdata_MC28)],
                                                    c("MC28", "RG65", "PSfull", "Agirre201", "SimLex665")))

# Loop for the computation of the metrics

nMeasures <- nrow(table_AvgMeasures_Spearman_SimDatasets)
nDatasets <- length(rawdataSimNounDatasets)

# We extract the best perfomring measre (Attract-reppel)

iBestSimMeasure <- 22

for (iDataset in 1:nDatasets)
{
  # We get the raw data of the next dataset
  
  rawdata <- rawdataSimNounDatasets[[iDataset]]

  # We get the human judgement vector and the raw similarity values
  # of the best performing measure in the current dataset
  
  humanJUgments <- rawdata[,2]
  rawbestMeasure <- rawdata[, iBestSimMeasure + 2]
  
  # We evaluate the Spearman correlation for each averaged measure in the current dataset
  
  for (iMeasure in 1:nMeasures)
  {
    # We compute the average similarity values corresponding to
    # the combination of the best performing measure with iMeasure
    
    currentMeasure <- rawdata[, iMeasure + 2]
    averaged_sim <- 0.5 * (currentMeasure + rawbestMeasure)      
    
    # We compute the Spearman correlation of the averaged measure
    
    table_AvgMeasures_Spearman_SimDatasets[iMeasure, iDataset] <- cor(humanJUgments, averaged_sim, method = "spearman")
  }
}

# We compute the average values per row and sort the rows

table_AvgMeasures_Spearman_SimDatasets <- cbind(table_AvgMeasures_Spearman_SimDatasets, Avg = rowMeans(table_AvgMeasures_Spearman_SimDatasets[1:nrow(table_AvgMeasures_Spearman_SimDatasets),]))
table_AvgMeasures_Spearman_SimDatasets <- mat.sort(table_AvgMeasures_Spearman_SimDatasets, ncol(table_AvgMeasures_Spearman_SimDatasets), decreasing = TRUE)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_AvgMeasures_Spearman_SimDatasets_rounded <- round(table_AvgMeasures_Spearman_SimDatasets, 3);

# We save all final assembled data tables 

write.csv(table_AvgMeasures_Spearman_SimDatasets, file = paste(outputDir, sep="","table_AvgMeasures_Spearman_SimDatasets.csv"))
write.csv(table_AvgMeasures_Spearman_SimDatasets_rounded, file = paste(outputAvgMetricFilesDir, sep="","table_AvgMeasures_Spearman_SimDatasets_rounded.csv"))


